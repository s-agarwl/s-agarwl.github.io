# robots.txt for s-agarwl.github.io

# Allow all search engines to crawl everything
User-agent: *
Disallow:  # Empty disallow means allow all

# Allow PDF files explicitly (optional, since Disallow is empty)
Allow: /*.pdf

# Specify the sitemap location
Sitemap: https://s-agarwl.github.io/sitemap.xml