<!DOCTYPE html><html lang="en"><head>
    <meta charset="UTF-8">
    <link rel="icon" type="image/svg+xml" href="/logo.svg">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&amp;display=swap" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Start Single Page Apps for GitHub Pages -->
    <script type="text/javascript">
      (function (l) {
        if (l.search[1] === '/') {
          var decoded = l.search
            .slice(1)
            .split('&')
            .map(function (s) {
              return s.replace(/~and~/g, '&');
            })
            .join('?');
          window.history.replaceState(null, null, l.pathname.slice(0, -1) + decoded + l.hash);
        }
      })(window.location);
    </script>
    <!-- End Single Page Apps for GitHub Pages -->

    <!-- Specific Hash URL Redirects -->
    <script type="text/javascript">
      (function () {
        // Map of specific hash URLs to their clean URL destinations
        var hashRedirectMap = {
          '#/publications': '/publications',
          '#/publications/': '/publications/', // Handle trailing slash version
          '#/publication/': '/publication/', // This will be a prefix match for publication URLs
          // '#/docs': '/docs',
          // Add more specific redirects as needed
        };

        // Get the current hash
        var hash = window.location.hash;
        console.log('Current hash:', hash); // Debug log

        // Check if this hash is in our redirect map
        if (hash) {
          // First check for exact matches
          if (hashRedirectMap[hash]) {
            console.log('Exact match found, redirecting to:', hashRedirectMap[hash]); // Debug log
            window.location.replace(window.location.origin + hashRedirectMap[hash]);
          }
          // Then check for prefix matches (for dynamic routes like publications)
          else {
            var matchFound = false;
            for (var prefix in hashRedirectMap) {
              if (hash.startsWith(prefix) && prefix.endsWith('/')) {
                var cleanPrefix = hashRedirectMap[prefix];
                var suffix = hash.substring(prefix.length);
                var redirectUrl = window.location.origin + cleanPrefix + suffix;
                console.log('Prefix match found, redirecting to:', redirectUrl); // Debug log
                window.location.replace(redirectUrl);
                matchFound = true;
                break;
              }
            }

            // If no match found but starts with #/, use the path directly
            if (!matchFound && hash.startsWith('#/')) {
              var cleanPath = hash.substring(1); // Remove the # character
              console.log('No match found, using direct path:', cleanPath); // Debug log
              window.location.replace(window.location.origin + cleanPath);
            }
          }
        }
      })();
    </script>
    <!-- End Specific Hash URL Redirects -->

    <title>Shivam Agarwal's Website - Data and AI Expert</title>
    <meta name="description" content="This is the website of Shivam Agarwal showcasing the portfolio of his work in data and AI.">
    <meta name="keywords" content="academic, research, publications, papers, scholarly work">
    <meta name="author" content="Shivam Agarwal">
    <meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow">
    <meta name="google-site-verification" content="L1QbO4CPLDpC06G3v_ZLEhlomt9fj0Vlj9DV_nCqGfc">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://s-agarwl.github.io/publications/Krause2023Visually">
    <meta property="og:title" content="Shivam Agarwal's Website - Data and AI Expert">
    <meta property="og:description" content="This is the website of Shivam Agarwal showcasing the portfolio of his work in data and AI.">
    <meta property="og:image" content="https://s-agarwl.github.io/logo.svg">

    <!-- Publication-specific metadata -->
    <meta name="citation_doi" content="10.1111/cgf.14805">
    <meta name="DC.identifier" content="doi:10.1111/cgf.14805">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://s-agarwl.github.io/publications/Krause2023Visually">
    <meta property="twitter:title" content="Shivam Agarwal's Website - Data and AI Expert">
    <meta property="twitter:description" content="This is the website of Shivam Agarwal showcasing the portfolio of his work in data and AI.">
    <meta property="twitter:image" content="https://s-agarwl.github.io/logo.svg">

    <!-- Google Analytics -->
    
    <!-- Google Analytics -->
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-8CMNGL0EH7"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'G-8CMNGL0EH7', {
        custom_map: {
          dimension1: 'paper_title',
          dimension2: 'paper_authors',
          dimension3: 'paper_year',
        },
      });
    </script>
  
    <script type="module" crossorigin="" src="/assets/index-CRTSx0q8.js"></script>
    <link rel="stylesheet" crossorigin="" href="/assets/index-Dd5h-51d.css">
  <link rel="canonical" href="https://s-agarwl.github.io/publications/Krause2023Visually"><meta property="DC.language" content="en"><script type="application/ld+json">{"@context":"https://schema.org","@type":"ScholarlyArticle","description":""}</script><script id="publication-data" type="application/json">{"entry":{"citationKey":"Krause2023Visually","entryType":"article","entryTags":{"author":"Cedric Krause, Shivam Agarwal, Michael Burch, and Fabian Beck","title":"Visually Abstracting Event Sequences as Double Trees Enriched with Category-Based Comparison","journal":"Computer Graphics Forum","volume":"42","number":"6","pages":"","year":"2023","data_type":"Event Sequences","application_domain":"Pattern Discovery, AI Research","visualization_type":"Graph Visualization, Visual Comparison","doi":"10.1111/cgf.14805","abstract":"Abstract Event sequence visualization aids analysts in many domains to better understand and infer new insights from event data. Analysing behaviour before or after a certain event of interest is a common task in many scenarios. In this paper, we introduce, formally define, and position double trees as a domain-agnostic tree visualization approach for this task. The visualization shows the sequences that led to the event of interest as a tree on the left, and those that followed on the right. Moreover, our approach enables users to create selections based on event attributes to interactively compare the events and sequences along colour-coded categories. We integrate the double tree and category-based comparison into a user interface for event sequence analysis. In three application examples, we show a diverse set of scenarios, covering short and long time spans, non-spatial and spatial events, human and artificial actors, to demonstrate the general applicability of the approach.","paperurl":"/publications/Krause2023Visually/Krause2023Visually.pdf","image":"/publications/Krause2023Visually/Krause2023Visually.PNG","demo":"https://vis-uni-bamberg.github.io/doubletrees/","github":"https://github.com/vis-uni-bamberg/vis-uni-bamberg.github.io/tree/main/doubletrees","video":"https://onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111%2Fcgf.14805&file=cgf14805-sup-0001-VideoS1.mp4"},"contentType":"publications","sectionPath":"/publications"},"config":{"site":{"title":"Shivam Agarwal's Website - Data and AI Expert","description":"This is the website of Shivam Agarwal showcasing the portfolio of his work in data and AI.","baseUrl":"https://s-agarwl.github.io","googleAnalyticsId":"G-8CMNGL0EH7","keywords":"academic, research, publications, papers, scholarly work","author":"Shivam Agarwal","googleSiteVerification":"L1QbO4CPLDpC06G3v_ZLEhlomt9fj0Vlj9DV_nCqGfc"},"sections":[{"id":"profile","title":"Profile","path":"/"},{"id":"publications","title":"Publications","path":"/publications","dataSource":"/data/pubs.bib","dataType":"bibtex"},{"id":"projects","title":"Projects","path":"/projects","dataSource":"/data/projects.json"},{"id":"mentorships","title":"Mentorships","path":"/mentorships","dataSource":"/mentorship/mentorship.json"},{"id":"my-story","title":"My Story","path":"/my-story"}]},"sectionConfig":{"id":"publications","path":"/publications","title":"Publications","sectionHeading":"Publications","template":"listOfItems","description":"Explore the collection of my publications. From groundbreaking research to practical applications, they reflect my passion for <b>unraveling complex datasets</b>, <b>visual analytics</b>, and <b>advancing AI understanding</b>.","dataSource":"/data/pubs.bib","dataType":"bibtex","bibtexFieldConfig":{"arrayFields":["keywords","data_type","application_domain","analysis_focus","visualization_type","test"],"arraySeparator":",","dateFields":["year","date"],"linkFields":["url","paperurl","slides","video","supplementary","demo","github","poster"],"additionalCitationFields":[]},"overviewVisualization":{"type":"KeywordCloud","enabled":true,"sourceFields":[{"field":"data_type","label":"Data Type"},{"field":"application_domain","label":"Application Domain"},{"field":"visualization_type","label":"Visualization Type"}],"fontSizes":{"min":11,"max":22},"maxVisibleKeywords":15},"display":{"list":{"fields":[{"field":"title","typeOfField":"Heading"},{"field":"authors","typeOfField":"AuthorList","label":"Authors"},{"field":"journal","typeOfField":"Text","variant":"italics_list"},{"field":"booktitle","typeOfField":"Text","variant":"italics_list"},{"field":"keywords","typeOfField":"Tags"},{"field":"awards","typeOfField":"Award"}],"showImage":true},"card":{"fields":[{"field":"title","typeOfField":"Heading","options":{"level":3}},{"field":"links","typeOfField":"PublicationLinks","options":{"showText":true}},{"field":"authors","typeOfField":"AuthorList","label":"Authors"},{"field":"year","typeOfField":"Text","label":"Year"},{"field":"abstract","typeOfField":"ExpandableMarkdown","options":{"limit":10},"label":"Abstract"},{"field":"keywords","typeOfField":"Tags"},{"field":"awards","typeOfField":"Award"}],"showImage":true},"detail":{"fields":[{"field":"title","typeOfField":"Heading","options":{"level":1}},{"field":"image","typeOfField":"Image"},{"field":"awards","typeOfField":"Award"},{"field":"authors","typeOfField":"AuthorList","label":"Authors"},{"field":"year","typeOfField":"Text","label":"Year"},{"field":"journal","typeOfField":"Text","label":"Venue","variant":"italics_detail"},{"field":"booktitle","typeOfField":"Text","label":"Venue","variant":"italics_detail"},{"field":"links","typeOfField":"PublicationLinks","options":{"showText":true}},{"field":"abstract","typeOfField":"ExpandableMarkdown","label":"Abstract","options":{"limit":30}},{"field":"keywords","typeOfField":"Tags","label":"Keywords"}],"actions":[{"type":"BibTeX","condition":"entryType"},{"type":"Video","condition":"links.video"}]}}}}</script></head>
  <body>
    <div id="prerendered-content"><h1>Visually Abstracting Event Sequences as Double Trees Enriched with Category-Based Comparison</h1><div class="metadata"><p><strong>Year: </strong>2023</p><p><strong>Journal: </strong>Computer Graphics Forum</p><p><strong>Authors: </strong>Cedric Krause, Shivam Agarwal, Michael Burch, and Fabian Beck</p><p><strong>DOI: </strong><a href="https://doi.org/10.1111/cgf.14805" target="_blank" rel="noopener noreferrer">10.1111/cgf.14805</a></p></div><h3>Abstract</h3><div class="markdown-content">Abstract Event sequence visualization aids analysts in many domains to better understand and infer new insights from event data. Analysing behaviour before or after a certain event of interest is a common task in many scenarios. In this paper, we introduce, formally define, and position double trees as a domain-agnostic tree visualization approach for this task. The visualization shows the sequences that led to the event of interest as a tree on the left, and those that followed on the right. Moreover, our approach enables users to create selections based on event attributes to interactively compare the events and sequences along colour-coded categories. We integrate the double tree and category-based comparison into a user interface for event sequence analysis. In three application examples, we show a diverse set of scenarios, covering short and long time spans, non-spatial and spatial events, human and artificial actors, to demonstrate the general applicability of the approach.</div><h2>Citation</h2><pre><code class="language-bibtex">@article{Krause2023Visually,
  author     = {Cedric Krause, Shivam Agarwal, Michael Burch, and Fabian Beck},
  title      = {Visually Abstracting Event Sequences as Double Trees Enriched with Category-Based Comparison},
  journal    = {Computer Graphics Forum},
  volume     = {42},
  number     = {6},
  pages      = {},
  year       = {2023},
  doi        = {10.1111/cgf.14805},
  paperurl   = {/publications/Krause2023Visually/Krause2023Visually.pdf},
  github     = {https://github.com/vis-uni-bamberg/vis-uni-bamberg.github.io/tree/main/doubletrees},
  demo       = {https://vis-uni-bamberg.github.io/doubletrees/},
  abstract   = {Abstract Event sequence visualization aids analysts in many domains to better understand and infer new insights from event data. Analysing behaviour before or after a certain event of interest is a common task in many scenarios. In this paper, we introduce, formally define, and position double trees as a domain-agnostic tree visualization approach for this task. The visualization shows the sequences that led to the event of interest as a tree on the left, and those that followed on the right. Moreover, our approach enables users to create selections based on event attributes to interactively compare the events and sequences along colour-coded categories. We integrate the double tree and category-based comparison into a user interface for event sequence analysis. In three application examples, we show a diverse set of scenarios, covering short and long time spans, non-spatial and spatial events, human and artificial actors, to demonstrate the general applicability of the approach.}
}</code></pre></div><div id="root"></div>

  

</body></html>